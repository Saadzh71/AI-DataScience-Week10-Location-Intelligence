{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ceuPdtVE9HbfybKFRQgS-Bqh7tMr-yk7","timestamp":1726818569199}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","#MDPs\n","\n"],"metadata":{"id":"6xgu4btu8M6X"}},{"cell_type":"markdown","source":["# Gridworld Example\n","In this example, we will create a simple 4x4 Gridworld. The agent can move up, down, left, or right. Some states will provide rewards (positive or negative), and some states will be terminal.\n"],"metadata":{"id":"a1V-uBWc8SGg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RkzpdHis6wH"},"outputs":[],"source":["\n","import numpy as np\n","\n","# ### 2.1. Define the Gridworld Environment\n","class Gridworld:\n","    def __init__(self, grid_size=(4, 4), terminal_states=[(0, 0), (3, 3)], rewards=None):\n","        \"\"\"\n","        Initialize the Gridworld environment.\n","        - grid_size: Dimensions of the grid.\n","        - terminal_states: Positions where the agent receives a reward and the episode ends.\n","        - rewards: Dictionary mapping state positions to rewards.\n","        \"\"\"\n","        self.grid_size = grid_size\n","        self.terminal_states = terminal_states\n","        self.rewards = rewards if rewards else {(0, 0): 1, (3, 3): 1}\n","        self.actions = ['up', 'down', 'left', 'right']\n","        self.grid = np.zeros(grid_size)\n","        for state, reward in self.rewards.items():\n","            self.grid[state] = reward\n","\n","    def is_terminal(self, state):\n","        return state in self.terminal_states\n","\n","    def get_next_state(self, state, action):\n","        i, j = state\n","        if action == 'up':\n","            return (max(i - 1, 0), j)\n","        elif action == 'down':\n","            return (min(i + 1, self.grid_size[0] - 1), j)\n","        elif action == 'left':\n","            return (i, max(j - 1, 0))\n","        elif action == 'right':\n","            return (i, min(j + 1, self.grid_size[1] - 1))\n","        return state\n"]},{"cell_type":"code","source":["# Implement the Value Iteration Algorithm\n","def value_iteration(env, gamma=0.9, theta=1e-6):\n","    \"\"\"\n","    Perform value iteration to find the optimal policy.\n","    - env: The Gridworld environment.\n","    - gamma: Discount factor.\n","    - theta: Convergence threshold.\n","    Returns:\n","    - policy: Optimal action for each state.\n","    - V: Optimal value function.\n","    \"\"\"\n","    V = np.zeros(env.grid_size)  # Initialize value function with zeros\n","    policy = np.full(env.grid_size, '', dtype=object)  # Initialize policy\n","\n","    while True:\n","        delta = 0  # Track changes in the value function\n","        for i in range(env.grid_size[0]):\n","            for j in range(env.grid_size[1]):\n","                state = (i, j)\n","                if env.is_terminal(state):\n","                    continue  # Skip terminal states\n","\n","                v = V[state]  # Current value\n","                action_values = []  # Store value of each action\n","\n","                # Evaluate each action\n","                for action in env.actions:\n","                    next_state = env.get_next_state(state, action)\n","                    reward = env.grid[next_state]\n","                    action_value = reward + gamma * V[next_state]\n","                    action_values.append(action_value)\n","\n","                best_action_value = max(action_values)\n","                V[state] = best_action_value  # Update value function\n","                delta = max(delta, abs(v - V[state]))  # Update delta for convergence\n","\n","                # Update policy with the best action\n","                best_action = env.actions[np.argmax(action_values)]\n","                policy[state] = best_action\n","\n","        if delta < theta:  # If change in value function is small, stop iterating\n","            break\n","\n","    return policy, V"],"metadata":{"id":"Qb3CTLPc8h1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the Value Iteration Algorithm\n","\n","# Create the Gridworld environment\n","gridworld_env = Gridworld()\n","\n","# Apply value iteration\n","optimal_policy, optimal_value_function = value_iteration(gridworld_env)"],"metadata":{"id":"PfPF3_gu8mQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display Results\n","print(\"Optimal Policy:\")\n","for row in optimal_policy:\n","    print(row)\n","\n","print(\"\\nOptimal Value Function:\")\n","print(np.round(optimal_value_function, 2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxJk0oA28qsd","executionInfo":{"status":"ok","timestamp":1726777339640,"user_tz":-180,"elapsed":10,"user":{"displayName":"Huriyyah Althenayan","userId":"03383746679280494688"}},"outputId":"54d94525-293a-4111-e96c-7405fec44cb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy:\n","['' 'left' 'left' 'down']\n","['up' 'up' 'up' 'down']\n","['up' 'up' 'down' 'down']\n","['up' 'right' 'right' '']\n","\n","Optimal Value Function:\n","[[0.   1.   0.9  0.81]\n"," [1.   0.9  0.81 0.9 ]\n"," [0.9  0.81 0.9  1.  ]\n"," [0.81 0.9  1.   0.  ]]\n"]}]}]}